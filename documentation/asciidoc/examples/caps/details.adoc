[[example-detail]]
= Detailed, custom DataFrame input

[abstract]
--
This section introduces some of the building blocks of a Cypher for Apache Spark application.
--

To introduce some of the building blocks of a Cypher for Apache Spark application, consider the custom DataFrame input example below.
The following parts will be explained in detail:

* <<example-detail-caps-session, Create a CAPS session>>.
* <<example-detail-prepare-dataframes, Prepare DataFrames>>.
* <<example-detail-create-node-relationship-tables, Create node and relationship tables>>.
* <<example-detail-create-graph, Create a graph>>.
* <<example-detail-run-query, Query the graph>>.
* <<example-detail-collect-results, Collect results>>.


The full example looks as follows:


[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=full-example]
----


[discrete]
[[example-detail-caps-session]]
== CAPS session

The first procedure in most CAPS applications is the creation of a CAPS session.
Here we create a local session.
For production, users will in most cases want to run in distributed mode.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=create-session]
----


[discrete]
[[example-detail-prepare-dataframes]]
== Prepare DataFrames

To show how to construct a property graph from DataFrames we first need some DataFrames.
Here we create the DataFrames as part of the example.
Normally, one would connect to one or more DataSources and pull DataFrames from there.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=prepare-dataframes]
----


[discrete]
[[example-detail-create-node-relationship-tables]]
== Create node and relationship tables

Having prepared the DataFrames we create node and relationship tables.
These tables wrap the DataFrames and describe their contained data.
Node and relationship mappings are used to explicitly define which DataFrame column stores which specific entity component.
The entity components are: identifiers, properties, node labels (optional), and relationship types.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=create-node-relationship-tables]
----

[discrete]
[[example-detail-create-graph]]
== Create a graph

Now that the node and relationship tables prepared we can create a property graph.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=create-graph]
----


[discrete]
[[example-detail-run-query]]
== Query the graph

Having created the graph we can query it with Cypher.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=run-query]
----


[discrete]
[[example-detail-collect-results]]
== Collect results

In the context of an example application, we can walk the query results and collect output that we can print.
Collecting results this way can often be very expensive, since the results are materialized locally.
Normally, the results would be streamed for further processing, or written to one or more DataSources.


[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=collect-results-typesafe]
----

There are two ways to collect results, as regards type enforcement.
Above we call `.as([String])` on each item in the result stream.
When forcing the type this way, values with non-matching types are discarded.

The other way to force the type is to use `.cast()` method, for example `.cast([string])`.
When forcing the type this way, encountering a value with a non-matching type will throw an exception.

[source, scala]
----
include::{examples-dir}/CustomDataFrameInputExample.scala[tags=collect-results-nontypesafe]
----

